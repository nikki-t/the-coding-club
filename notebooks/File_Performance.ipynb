{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating the Efficiency of File Formats for SWOT Data on the Cloud ‚òÅÔ∏è\n",
    "As terrabytes of data begin to stream into the cloud from the SWOT mission, storing and providing access to this data in the cloud has become a big priority. A new file format is needed to provide access to the data that is currently sitting on the cloud because the original format, .nc, is not efficient enough to use on cloud servers. With candidates such as Zarr and JSON via Kerchunk, this project is centered around the ease of writing, loading, and reading data to and from these various file formats.\n",
    "\n",
    "#### Eric Pham - Jet Propulsion Laboratory üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import statements.'''\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "import xarray as xr\n",
    "import zarr\n",
    "import s3fs\n",
    "import os\n",
    "import kerchunk.hdf\n",
    "from kerchunk.combine import MultiZarrToZarr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fsspec\n",
    "import requests\n",
    "import cartopy.crs as ccrs\n",
    "from matplotlib import pyplot as plt\n",
    "from os import path\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "\n",
    "hvplot.extension('bokeh')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0: Bucket Access and Dask ü™£\n",
    "\n",
    "A dask client is used to parallelize the loading and reading of the data. The AWS key ID and secret key are also provided to allow access to data in the bucket. \n",
    "\n",
    "Something to make note of is that most of the processes were run on a small server notebook with a 4GB memory limit. This meant that in terms generating, loading, and plotting, certain file types were limited. In particular, using Matplotlib to display the plots overloaded memory several times so it was not a viable option for plotting data. Generating large Zarr and Kerchunk files also overloaded the server, so these processes are best done locally if large amounts of data are involved. Loading large amounts of NetCDF and Individual Kerchunk data also overloaded memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Daskhub client initialization.'''\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=2)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''AWS credentials.'''\n",
    "\n",
    "'''\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = #Enter Key\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = #Enter Secret Key\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=False, key= #Enter Key, \n",
    "secret= #Enter Secret Key)\n",
    "df = pd.read_csv(#Enter Name of Dataset).drop(columns=\"Unnamed: 0\")\n",
    "local_df = df[df[\"Environment\"]==\"Local\"]\n",
    "cloud_df = df[df[\"Environment\"]==\"Cloud\"]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: File Generation üìÅ\n",
    "The code blocks below provide examples on how to write the original file into a new format. Data about the speed of writing is also provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Writing multiple granules to a zarr file.'''\n",
    "\n",
    "# Edit this section and replace it with the s3path which contains the data.\n",
    "num_gran = 23\n",
    "s3path_nc = f's3://podaac-swot-science-sandbox/data_{num_gran}_netcdf/*'\n",
    "netcdf_files = s3.glob(s3path_nc)\n",
    "data_files = [s3.open(file) for file in netcdf_files]\n",
    "\n",
    "# Opens an xarray dataset of all the granules concatenated by num_lines.\n",
    "com_data = xr.open_mfdataset(data_files, concat_dim=\"num_lines\", \n",
    "engine=\"h5netcdf\", combine=\"nested\")\n",
    "\n",
    "# Writing the data to a zarr file. Edit the name of the ouput file. \n",
    "compressor = zarr.Blosc(cname='zstd', clevel=3)\n",
    "encoding = {vname: {'compressor': compressor} for vname in com_data.data_vars}\n",
    "#com_data.to_zarr(f\"test_data_{num_gran}_zarr_combine\", consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Writing individual kerchunk files.'''\n",
    "\n",
    "# Edit this section and replace it with the s3path which contains the data.\n",
    "num_gran = 23\n",
    "s3path_nc = f's3://podaac-swot-science-sandbox/data_{num_gran}_netcdf/*'\n",
    "netcdf_files = s3.glob(s3path_nc)\n",
    "\n",
    "\n",
    "# Dictionary of reference JSONs.\n",
    "singles = []\n",
    "\n",
    "# Writes reference files for each individual granule.\n",
    "# Time to write is approximately 15 seconds per granule.\n",
    "so = dict(anon=False, default_fill_cache=False, default_cache_type='first')\n",
    "\n",
    "for u in [\"s3://\" + f for f in netcdf_files]:\n",
    "\n",
    "    with fsspec.open(u, **so) as inf:\n",
    "\n",
    "        h5chunks = kerchunk.hdf.SingleHdf5ToZarr(inf, u, inline_threshold=0)\n",
    "        singles.append(h5chunks.translate()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Writing a combined kerchunk file.'''\n",
    "\n",
    "# This line of code creates a new dimension to concatenate all of the granules \n",
    "# along and creates a reference file which can be used to access multiple files.\n",
    "mzz = MultiZarrToZarr(singles, remote_protocol=\"s3\", \n",
    "remote_options={'anon': False}, coo_map={\"z\": \"INDEX\"}, concat_dims=[\"z\"])\n",
    "out = mzz.translate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing is an overlooked but equally important step when it comes to determining a good file format. If a file is relatively easy to load but takes a lot of time to create this can create a backlog when it comes to converting the files. And once the files are created, storing the files is another thing to take note of since these files will be sitting on the cloud, where storage is not free.\n",
    "\n",
    "**NOTE:** Something important to note is that the writing times were recorded for only 23 granules as more data would overload the memory limit of the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data_1 = local_df.groupby(\"File Type\").sum().reset_index()\\\n",
    ".sort_values(by=\"Writing Time (s)\", ascending=False)\n",
    "data_1[\"Projected Writing Time (min)\"] = data_1[\"Writing Time (s)\"] / 731 \n",
    "* 10000 / 60\n",
    "\n",
    "data_1.hvplot.bar(x=\"File Type\", y=\"Projected Writing Time (min)\", \n",
    "color=\"File Type\", cmap=[\"chocolate\", \"gold\", \"palegreen\", \"dodgerblue\"], \n",
    "title=\"Projected Time (min) to Write 10000 Granules Locally\", legend=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data_1[\"Projected Size (GB)\"] = data_1[\"Size (MB)\"] / 731 * 1000\n",
    "data_1.hvplot.bar(x=\"File Type\", y=\"Projected Size (GB)\", \n",
    "color=\"File Type\", cmap=[\"chocolate\", \"gold\", \"palegreen\", \"dodgerblue\"], \n",
    "title=\"Projected Size (GB) of 10000 Granules on Cloud\", legend=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Individual Kerchunk** and **Combined Kerchunk** file is the clear winner in this category, sporting the fastest writing time while also taking up the least disk space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Loading... ‚åõ\n",
    "Once the data is successfully written, it can be opened following the code blocks below. Data about the speed of loading is also provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loading a netcdf.'''\n",
    "\n",
    "# Edit this section and replace it with the s3path which contains the data.\n",
    "s3path_nc = f's3://podaac-swot-science-sandbox/data_{num_gran}_netcdf/*'\n",
    "netcdf_files = s3.glob(s3path_nc)\n",
    "netcdf_fileset = [s3.open(file) for file in netcdf_files]\n",
    "\n",
    "# Loads the data using xarray.\n",
    "netcdf_file = xr.open_mfdataset(netcdf_fileset, engine='h5netcdf', \n",
    "combine='nested', chunks={}, concat_dim=\"num_lines\", decode_times=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loading a zarr.'''\n",
    "\n",
    "# Edit this section and replace it with the s3path which contains the data.\n",
    "gran = 23\n",
    "s3path_zarrc = f's3://podaac-swot-science-sandbox/data_{gran}_zarr_combine/'\n",
    "store = s3fs.S3Map(root=s3path_zarrc, s3=s3, check=False)\n",
    "\n",
    "# Loads the data and chunks it. Chunks are based off of simulated SWOT data.\n",
    "zarr_combine_file = xr.open_zarr(store=store, \n",
    "consolidated=True).chunk({\"num_lines\": 9864, \"num_pixels\": 71})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loading individual kerchunk files.'''\n",
    "\n",
    "# List of opened files.\n",
    "cat_files = []\n",
    "\n",
    "# Enter AWS credentials here to access files.\n",
    "remote = {\"anon\": False, \"key\": , \n",
    "    \"secret\": }\n",
    "\n",
    "# Individually opens reference files.\n",
    "for s in singles:\n",
    "\n",
    "    # File access options.\n",
    "    storage_i = {\"fo\": s, \"remote_protocol\": \"s3\", \"remote_options\": remote} \n",
    "    backend_i = {\"storage_options\": storage_i, \"consolidated\": False}\n",
    "\n",
    "    # Reading the JSONs stored in singles. Alternatively, the kerchunk files can \n",
    "    # be written to a path and read in that way.\n",
    "    cat_files.append(xr.open_dataset(\"reference://\", engine=\"zarr\", chunks={}, \n",
    "    backend_kwargs=backend_i))\n",
    "\n",
    "\n",
    "\n",
    "# Concatenates all of the individually opened kerchunk files into a dataset.\n",
    "kerchunk_files = xr.concat(cat_files, \"num_lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loading combined kerchunk file.'''\n",
    "\n",
    "# Uses AWS credentials to access file.\n",
    "remote = {\"anon\": False, \"key\": , \n",
    "    \"secret\": }\n",
    "storage_c = {\"fo\": out, \"remote_protocol\": \"s3\", \"remote_options\": remote}\n",
    "backend_c = {\"storage_options\": storage_c, \"consolidated\": False}\n",
    "\n",
    "# Opens the dataset using the combined kerchunk reference file.\n",
    "combined_kerchunk_files = xr.open_dataset(\"reference://\", engine=\"zarr\", \n",
    "chunks={}, backend_kwargs=backend_c)\n",
    "combined_kerchunk_files = combined_kerchunk_files.assign_coords(z=np.arange(23))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, once a file is written, it does not need to be adjusted any further. The same cannot be said for loading, which makes it one of the most important metrics of performance in terms of evaluating a files performance. If the overhead time to write a file is ~5 seconds but takes 30 minutes to load everytime the dataset is needed, then it is not the most optimal means of storing data. \n",
    "\n",
    "All of the data collected was gathered on a cloud environment, using a notebook with a 4GB memory maximum. NetCDF and Individual Kerchunk both exceeded the memory maximum for 589 granules so no data is available for those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data_2 = cloud_df[cloud_df[\"Num Granules\"]!=589][[\"File Type\", \n",
    "\"Loading 5 Times (s)\", \"Loading 10 Times (s)\"]].groupby(\"File Type\").sum()\n",
    "data_2[\"Average Loading Time (s)\"] = (data_2[\"Loading 5 Times (s)\"] \n",
    "+ data_2[\"Loading 10 Times (s)\"]) / 15\n",
    "data_2 = data_2 / 142 * 1000\n",
    "data_2[\"Order\"] = [3, 2, 0, 1]\n",
    "data_2 = data_2.sort_values(by=\"Order\").reset_index()\n",
    "\n",
    "data_2.hvplot.bar(x=\"File Type\", y=\"Average Loading Time (s)\", \n",
    "color=\"File Type\", cmap=[\"chocolate\", \"gold\", \"palegreen\", \"dodgerblue\"], \n",
    "title=\"Projected Loading Time (s) for 1000 Granules on Cloud\", legend=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Zarr** and **Combined Kerchunk** are the file formats that allow data to be access the most quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Reading and Plotting üìç\n",
    "\n",
    "Once the data is lazily loaded, ensuring that using the data is easy and efficient is the important final step. Blocks of code below show how to use hvplot to display data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Select a file format from above to plot.'''\n",
    "\n",
    "# Sets the dataset to an already loaded dataset.\n",
    "dataset = zarr_combine_file\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Plotting figures using hvplot.'''\n",
    "\n",
    "# Plots simulated_true_ssh_karin.\n",
    "dataset.simulated_true_ssh_karin.hvplot.points('longitude', 'latitude', \n",
    "aggregator=\"mean\", crs=ccrs.PlateCarree(), projection=ccrs.PlateCarree(), \n",
    "project=True, geo=True, rasterize=True, coastline=True, frame_width=800, \n",
    "dynamic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring that data is not only quick to load but also read from is important for the end-product. As of right now, the memory limit prevents plotting more ~150 granules so only a small amount of granules were test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data_3 = cloud_df[cloud_df[\"Num Granules\"]==23]\n",
    "\n",
    "data_3.hvplot.bar(x=\"File Type\", y=\"Loading + Hvplot (s)\", \n",
    "color=\"File Type\", cmap=[\"chocolate\", \"gold\", \"palegreen\", \"dodgerblue\"], \n",
    "title=\"Time to Load + Hvplot Plot (s) 23 Granules on Cloud\", legend=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Zarr**, **Individual Kerchunk**, and **Combined Kerchunk** files perform equally. Compared to the NetCDF format, the difference is clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Conclusion ‚úÖ\n",
    "\n",
    "After examining the data from all four file formats one thing remains very clear; the legacy NetCDF is not made to perform in cloud environments. However, selecting the optimal file format becomes tricky when all things are considered. The two front-runners are the **Zarr** and **Combined Kerchunk** formats. \n",
    "\n",
    "While the Zarr format is extremely space inefficient, it performs well in all other categories and also meshes extremely well with any existing code. It does not alter the structure of the data in any way and works well with the existing PO.DAAC tutorial code.\n",
    "\n",
    "On the otherhand, the Combined Kerchunk format is extremely space efficient and loads and reads extremely fast. While it seems that there are no downsides to this, the Combined Kerchunk requires that a new dimension is added to the data to concatenate all the granules together. This slightly alters the structure of the data, making it difficult to plot using the existing code provided on the PO.DAAC cookbook.\n",
    "\n",
    "These things must be considered in conjunction when making a decision as to which file format is most efficient. Zarr most closely mirrors the legacy format and is easy to manufacture whilst the Kerchunk library may pose difficults for first time users as well as slightly changing the structure of the data.\n",
    "\n",
    "\n",
    "List of Resources:\n",
    "- https://podaac.github.io/tutorials/external/Direct_Access_SWOT_sim_Oceanography.html\n",
    "- https://fsspec.github.io/kerchunk/test_example.html\n",
    "- https://github.com/lsterzinger/cloud-optimized-satellite-data-tests\n",
    "- https://ntrs.nasa.gov/api/citations/20200001178/downloads/20200001178.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
